{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c0cac46-931c-4190-ac2f-92e593988045",
   "metadata": {},
   "source": [
    "# Session 3: Spring 2026\n",
    "\n",
    "Big Data Algorithms\n",
    "\n",
    "## Outline\n",
    "\n",
    "Review from the last session:\n",
    "\n",
    "-   LSH family for minhash signatures\n",
    "\n",
    "-   AND-OR and OR-AND amplification\n",
    "\n",
    "-   An example with text data\n",
    "\n",
    "-   LSH Families for Cosine, Hamming and Euclidean distances\n",
    "\n",
    "-   Data Streams:\n",
    "\n",
    "    -   Reservoir Sampling\n",
    "    -   Counting unique items: the FM Algorithm\n",
    "\n",
    "## Minhashing: LSH family for Jaccard distance\n",
    "\n",
    "## AND, OR, AND-OR and OR-AND amplification\n",
    "\n",
    "## Pairing probabilities\n",
    "\n",
    "With $n$ signatures arranged in $b$ bands with $r$ rows per band\n",
    "($n = r \\cdot b)$), the probabilities for two documents with similarity\n",
    "$s$ becoming a **candidate pair** are:\n",
    "\n",
    "-   **AND-OR**: $1 - (1-s^r)^b$\n",
    "\n",
    "-   **OR-AND**: $(1- (1-s)^r)^b$\n",
    "\n",
    "## Exercise (submit your code on Canvas)\n",
    "\n",
    "Suppose we wish to amplify a $(0.2, 0.6, 0.8, 0.4)$-sensitive family.\n",
    "\n",
    "How many signatures, rows and bands with what kind (either AND-OR or\n",
    "OR_AND) of amplification can achieve each of the following goals?\n",
    "\n",
    "-   Reduce the false positive rate from 0.4 to less than 0.15\n",
    "\n",
    "-   Reduce the false negative rate from 0.2 to less than 0.1\n",
    "\n",
    "-   Reduce both rates simultaneously to less than 0.05\n",
    "\n",
    "## Python Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a40617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Base rates from the (0.2, 0.6, 0.8, 0.4)-sensitive family\n",
    "p_near = 0.8   # true positive rate (TPR) for similar pairs\n",
    "p_far  = 0.4   # false positive rate (FPR) for dissimilar pairs\n",
    "\n",
    "def or_and(p, r, b):\n",
    "    \"\"\"\n",
    "    OR of b bands, each band is AND of r rows:\n",
    "    p' = 1 - (1 - p^r)^b\n",
    "    \"\"\"\n",
    "    return 1.0 - (1.0 - (p ** r)) ** b\n",
    "\n",
    "def and_or(p, r, b):\n",
    "    \"\"\"\n",
    "    AND of b groups, each group is OR of r rows:\n",
    "    p' = (1 - (1 - p)^r)^b\n",
    "    \"\"\"\n",
    "    return (1.0 - (1.0 - p) ** r) ** b\n",
    "\n",
    "def rates_for_scheme(scheme_fn, r, b):\n",
    "    \"\"\"Return amplified (TPR, FPR, FNR) for given scheme and params.\"\"\"\n",
    "    tpr = scheme_fn(p_near, r, b)\n",
    "    fpr = scheme_fn(p_far,  r, b)\n",
    "    fnr = 1.0 - tpr\n",
    "    return tpr, fpr, fnr\n",
    "\n",
    "def find_params(scheme_fn, max_r=40, max_b=60, fpr_max=None, fnr_max=None):\n",
    "    \"\"\"\n",
    "    Find (r,b) meeting constraints. Minimizes total rows used = r*b,\n",
    "    then breaks ties by smaller b, then smaller r.\n",
    "    \"\"\"\n",
    "    best = None\n",
    "    best_key = None\n",
    "\n",
    "    for r in range(1, max_r + 1):\n",
    "        for b in range(1, max_b + 1):\n",
    "            tpr, fpr, fnr = rates_for_scheme(scheme_fn, r, b)\n",
    "\n",
    "            if fpr_max is not None and not (fpr < fpr_max):\n",
    "                continue\n",
    "            if fnr_max is not None and not (fnr < fnr_max):\n",
    "                continue\n",
    "\n",
    "            key = (r * b, b, r)  # cost metric: total signatures/rows, etc.\n",
    "            if best is None or key < best_key:\n",
    "                best = (r, b, tpr, fpr, fnr)\n",
    "                best_key = key\n",
    "\n",
    "    return best\n",
    "\n",
    "def pretty_result(name, res):\n",
    "    if res is None:\n",
    "        print(f\"{name}: no solution found in search bounds.\")\n",
    "        return\n",
    "    r, b, tpr, fpr, fnr = res\n",
    "    print(f\"{name}: r={r}, b={b}, total rows r*b={r*b}\")\n",
    "    print(f\"  TPR={tpr:.6f}  FPR={fpr:.6f}  FNR={fnr:.6f}\")\n",
    "\n",
    "# --------------------------\n",
    "# Goal A: FPR < 0.15\n",
    "# --------------------------\n",
    "print(\"GOAL A: Reduce FPR from 0.4 to < 0.15\\n\")\n",
    "pretty_result(\"OR-AND best\", find_params(or_and, fpr_max=0.15))\n",
    "pretty_result(\"AND-OR best\", find_params(and_or, fpr_max=0.15))\n",
    "\n",
    "# --------------------------\n",
    "# Goal B: FNR < 0.10\n",
    "# --------------------------\n",
    "print(\"\\nGOAL B: Reduce FNR from 0.2 to < 0.10\\n\")\n",
    "pretty_result(\"OR-AND best\", find_params(or_and, fnr_max=0.10))\n",
    "pretty_result(\"AND-OR best\", find_params(and_or, fnr_max=0.10))\n",
    "\n",
    "# --------------------------\n",
    "# Goal C: BOTH < 0.05\n",
    "# --------------------------\n",
    "print(\"\\nGOAL C: Reduce BOTH FPR and FNR to < 0.05\\n\")\n",
    "pretty_result(\"OR-AND best\", find_params(or_and, fpr_max=0.05, fnr_max=0.05))\n",
    "pretty_result(\"AND-OR best\", find_params(and_or, fpr_max=0.05, fnr_max=0.05))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4c0ef33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "rng = np.random.default_rng()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79995b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "class UHF:\n",
    "    \"\"\"A factory for producing a universal family of hash functions\"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def is_prime(k):\n",
    "        if k%2==0:\n",
    "            return False\n",
    "        for i in range(3, int(math.sqrt(k)), 2):\n",
    "            if k%i == 0:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def __init__(self, n):\n",
    "        \"\"\"Universe size is n\"\"\"\n",
    "        self.n = n\n",
    "        if n%2==0:\n",
    "            m = n+1\n",
    "        else:\n",
    "            m = n+2\n",
    "        while not(UHF.is_prime(m)):\n",
    "            m = m+2\n",
    "        self.p = m\n",
    "\n",
    "    def make_hash(self, m):\n",
    "        \"\"\"Return a random hash function\n",
    "\n",
    "        m: table size\n",
    "        \"\"\"\n",
    "        a = random.randint(1,self.p-1)\n",
    "        b = random.randint(0,self.p-1)\n",
    "        return lambda k: ((a*k+b)%self.p)%m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a2e76e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10000000\n",
    "uhf = UHF(n)\n",
    "uhf.p\n",
    "m=5000\n",
    "make_hash = uhf.make_hash(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c4002e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = int(np.random.random()*100000)\n",
    "q = int(np.random.random()*100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ca575cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hashes = [uhf.make_hash(m) for _ in range(50000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "36a7e2d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(i(p) == i(q) for i in hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5aae918",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a715fcde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fffba4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b846c0b2-8f21-4895-a4ba-ee979641d78f",
   "metadata": {},
   "source": [
    "## Test the Universal Hash Family (Exercise)\n",
    "\n",
    "Set up and evaluate an experiment to verify that the family defined\n",
    "above is indeed a universal family. Use a reasonable universe of keys,\n",
    "say integers in `range(1000000)` and a hash table of size $5000$.\n",
    "\n",
    "-   Fix two keys at random\n",
    "-   Generate a lot of hash functions and see how many of them collide\n",
    "\n",
    "## Shingling\n",
    "\n",
    "To compare **text** data, we create **$k$-shingles** from the data\n",
    "(sometimes called **$k$-grams**).\n",
    "\n",
    "> all substrings consisting of $k$ consecutive characters from the text!\n",
    "\n",
    "Each text document is characterized by its $k$-shingles and their\n",
    "*frequency*. We can also just use binary indicators for the shingles.\n",
    "\n",
    "We look for similaries between actual text documents: in this case, a\n",
    "collection of term papers (extract the zip file `med_doc_set.zip` into a\n",
    "folder called `med_doc_set`).\n",
    "\n",
    "## Matrix Representation Of Data\n",
    "\n",
    "A useful builtin module called `pathlib` can be used to work with files\n",
    "on the filesystem in Python. For example, let’s collect all the files\n",
    "are our sub-folder `med_doc_set`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f2e5c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "med_doc_set = Path('med_doc_set/med_doc_set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9293d9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range:1:3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01235791-3ff3-4ba3-81fa-c23128147604",
   "metadata": {},
   "source": [
    "## Scikit-Learn text data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b156738d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b08ecfc-8d9d-42cb-af8b-2767b3a23c35",
   "metadata": {},
   "source": [
    "The `CountVectorizer` and `TfidfVectorizer` classes create **models**:\n",
    "objects that can be used to **fit** data for further analysis. Once\n",
    "fitted, the resulting model acquires a **term** vocabulary (the\n",
    "features) that can be **transformed** to a **document-term** matrix, a\n",
    "starting point for most mining/machine-learning algorithms.\n",
    "\n",
    "`sklearn` also allows a model to combine the fit and transform steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d8e1349",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      1\u001b[39m files_iterator = med_doc_set.glob(\u001b[33m'\u001b[39m\u001b[33m*.txt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m cv = CountVectorizer(\u001b[38;5;28minput\u001b[39m=\u001b[33m'\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33miso-8859-1\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      3\u001b[39m                      decode_error=\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m, analyzer=\u001b[33m'\u001b[39m\u001b[33mword\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      4\u001b[39m                      ngram_range=(\u001b[32m1\u001b[39m,\u001b[32m2\u001b[39m), stop_words=\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      5\u001b[39m                      binary=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m cv_m = \u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/base.py:1336\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1329\u001b[39m     estimator._validate_params()\n\u001b[32m   1331\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1332\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1333\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1334\u001b[39m     )\n\u001b[32m   1335\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1336\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1386\u001b[39m, in \u001b[36mCountVectorizer.fit_transform\u001b[39m\u001b[34m(self, raw_documents, y)\u001b[39m\n\u001b[32m   1378\u001b[39m             warnings.warn(\n\u001b[32m   1379\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mUpper case characters found in\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1380\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m vocabulary while \u001b[39m\u001b[33m'\u001b[39m\u001b[33mlowercase\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1381\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m is True. These entries will not\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1382\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m be matched with any documents\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1383\u001b[39m             )\n\u001b[32m   1384\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m vocabulary, X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1388\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.binary:\n\u001b[32m   1389\u001b[39m     X.data.fill(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/feature_extraction/text.py:1292\u001b[39m, in \u001b[36mCountVectorizer._count_vocab\u001b[39m\u001b[34m(self, raw_documents, fixed_vocab)\u001b[39m\n\u001b[32m   1290\u001b[39m     vocabulary = \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[32m   1291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[32m-> \u001b[39m\u001b[32m1292\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1293\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1294\u001b[39m         )\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m indptr[-\u001b[32m1\u001b[39m] > np.iinfo(np.int32).max:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[32m   1297\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[31mValueError\u001b[39m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "files_iterator = med_doc_set.glob('*.txt')\n",
    "cv = CountVectorizer(input='filename', encoding='iso-8859-1',\n",
    "                     decode_error='ignore', analyzer='word',\n",
    "                     ngram_range=(1,2), stop_words='english',\n",
    "                     binary=True)\n",
    "cv_m = cv.fit_transform(files_iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cbff51-6d0c-47bb-a490-445b917e84bb",
   "metadata": {},
   "source": [
    "`sklearn` models have attributes whose names end in an underscore: best\n",
    "to read the documentation! The document-term matrix has terms as columns\n",
    "and documents as rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "20cc2cd8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cv_m' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m num_docs, num_features = \u001b[43mcv_m\u001b[49m.shape\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(num_docs, num_features)\n",
      "\u001b[31mNameError\u001b[39m: name 'cv_m' is not defined"
     ]
    }
   ],
   "source": [
    "num_docs, num_features = cv_m.shape\n",
    "print(num_docs, num_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4845befc-03e6-4efa-aee3-617470e8c53d",
   "metadata": {},
   "source": [
    "## Minhashing with Multiple Signatures & Amplification!\n",
    "\n",
    "Consider Jaccard similarity for now:\n",
    "\n",
    "-   Generate 20 different signatures for the documents: signatures\n",
    "    matrix has signatures as rows and documents as columns\n",
    "\n",
    "-   Arrange signatures in 5 **bands**, with each band containing 4\n",
    "    **rows**.\n",
    "\n",
    "-   **Candidate Pair:** Any pair of documents whose signatures agree in\n",
    "    **every** row in **some** band.\n",
    "\n",
    "## Find Candidate Pairs\n",
    "\n",
    "1.  Hash documents to *buckets* such that similar documents are likely\n",
    "    to hash to the same bucket!\n",
    "\n",
    "2.  **Actually compare features** of candidate pairs\n",
    "\n",
    "Benefits: Instead of $O(n^2)$ comparisons, we only need $O(n)$\n",
    "comparisons to find *approximately* similar documents!\n",
    "\n",
    "## General Approach\n",
    "\n",
    "-   Choose distance measure for items\n",
    "\n",
    "-   Create a matrix of signatures using an **appropriate LSH family**\n",
    "\n",
    "-   Construct candidate pairs applying the LSH banding technique\n",
    "\n",
    "-   Choose a threshold fraction $t$ for *similarity* of items in order\n",
    "    for them to be regarded as a *true pair*.\n",
    "\n",
    "-   Check if the signatures for the candidate pairs match in at least a\n",
    "    fraction $t$, or if pairs are sufficiently similar, do a more\n",
    "    fine-grained check.\n",
    "\n",
    "## Cosine Distance\n",
    "\n",
    "Distances equals angle (between 0 and $\\pi$) between points in Euclidean\n",
    "space\n",
    "\n",
    "-   Can be computed with dot products of the vectors representing the\n",
    "    points\n",
    "\n",
    "-   Distance is a metric\n",
    "\n",
    "> Note: `scipy` treats the cosine distance as the **cosine** of the\n",
    "> angle between points. Hence `scipy` cosine “distance’ is not a\n",
    "> distance metric! However, it is convenient because no `acos` (inverse\n",
    "> cosine) computations are needed.\n",
    "\n",
    "## LSH Family for Cosine Distance\n",
    "\n",
    "-   Data is a collection of $n$-dimensional points (treated as vectors)\n",
    "\n",
    "-   A “hash function” in the family corresponds to a **random vector**;\n",
    "    the hash function applied to a point is the **sign of the dot\n",
    "    product** of the point with the random vector. For angles\n",
    "    $d_1 < d_2$ in radians, we get a\n",
    "\n",
    "> $(d_1, d_2, (1-\\frac{d_1}{\\pi}), (1-\\frac{d_2}{\\pi}))$-sensitive\n",
    "> family\n",
    "\n",
    "**Sketch:** A vector with coordinates $\\pm 1$. These are reasonable\n",
    "approximations to the full LSH family in high dimensions.\n",
    "\n",
    "## Term Frequency Inverse-Document Frequency\n",
    "\n",
    "-   Term Frequency ($D_t$): **number** of times a term $t$ occurs in a\n",
    "    document (`CountVectorizer` does that)\n",
    "\n",
    "-   Document Frequency ($N_t$): **number** of documents that contain a\n",
    "    reference to a term $t$\n",
    "\n",
    "$\\textrm{TF-IDF} = D_t * \\log \\frac{1+N}{1+N_t}$\n",
    "\n",
    "**TF-IDF** gives more weight to less-frequent terms, and less weight to\n",
    "more-frequent terms. Here is a `TfidfVectorizer` model for the same\n",
    "document set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb07c601",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfv = TfidfVectorizer(input='filename', encoding='iso-8859-1',\n",
    "                      decode_error='ignore', analyzer='word',\n",
    "                      ngram_range=(2,2), stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c46c60-72d8-45a0-b0e1-9d6978f6805c",
   "metadata": {},
   "source": [
    "## Hamming Distance\n",
    "\n",
    "Distance between 0/1-valued (binary) vectors of length $n$\n",
    "\n",
    "-   equals the number of bit positions that differ, or can also be\n",
    "    defined as the *fraction* of bit positions that differ; i.e., it is\n",
    "    integer-valued between 0 and $n$ (when un-normalized) or $\\leq 1$\n",
    "    (when normalized by $n$).\n",
    "\n",
    "-   it is a metric distance\n",
    "\n",
    "## LSH Family for Hamming Distance\n",
    "\n",
    "-   Data consists of $n$ dimensional **binary** vectors with positions\n",
    "    indexed from 0 through $(n-1)$.\n",
    "\n",
    "-   A “hash function” in the family corresponds to a *projection*: the\n",
    "    projection $h_i$ for $0 \\leq i \\leq n-1$ extracts the $i^{th}$\n",
    "    (least significant) bit of a vector.\n",
    "\n",
    "*Normalized* Hamming distance $\\implies$\n",
    "$(d_1, d_2, (1- \\frac{d_1}{n}), (1-\\frac{d_2}{n}))$-sensitive family.\n",
    "\n",
    "## Euclidean Distances\n",
    "\n",
    "-   $L_1$ distance (also called **Manhattan distance**):\n",
    "\n",
    "$L_1(p,q) = \\sum_{i=0}^{n} |p_i - q_i|$\n",
    "\n",
    "-   $L_k$ distance: (the $k=2$ case is the familar distance norm)\n",
    "\n",
    "$L_k(p, q) = \\sqrt[k]{\\sum_{i=0}^{n-1} |p_i - q_i|^k}$\n",
    "\n",
    "-   $L_{\\infty}$ distance:\n",
    "\n",
    "$L_{\\infty}(p, q) = max_{i} |p_i - q_i|$\n",
    "\n",
    "## Example: LSH family for Euclidean $L_2$ distance\n",
    "\n",
    "-   Data is a collection of $2$-dimensional points\n",
    "\n",
    "-   A “hash function” in the family corresponds to a **random line**\n",
    "    that is divided into numbered segments (**buckets**) of length $a$\n",
    "\n",
    "-   The hash application is the bucket number reached when a\n",
    "    **perpendicular** is dropped from the point onto the line!\n",
    "\n",
    "> This yields a $(\\frac{a}{2}, 2a, \\frac{1}{2}, \\frac{1}{3})$-sensitive\n",
    "> hash family.\n",
    "\n",
    "## Data Structures/Algorithms for Data Streams\n",
    "\n",
    "Very large volumes of data that requires *immediate* (real-time)\n",
    "processing or is too large to be *analyzed in time* from archives.\n",
    "\n",
    "-   Sensor data (e.g. weather information, or sensors used in IOT\n",
    "    applications)\n",
    "\n",
    "-   Satellite data\n",
    "\n",
    "-   Data involved in cyber-security, e.g. IP traffic on a router\n",
    "\n",
    "-   Stock price fluctuations\n",
    "\n",
    "-   Identifying frequent search queries\n",
    "\n",
    "-   Identifying popular products\n",
    "\n",
    "## Data Summaries and Sampling\n",
    "\n",
    "We often need:\n",
    "\n",
    "-   statistical information about *sliding windows*\n",
    "\n",
    "-   extraction of *representative* or *reliable* samples: subset\n",
    "    selection\n",
    "\n",
    "-   filters to select samples\n",
    "\n",
    "-   approximate counts of distinct elements\n",
    "\n",
    "In all these applications, **hashing** is the key!\n",
    "\n",
    "## Reservoir Sampling\n",
    "\n",
    "We wish to choose a subset (the reservoir) of size $k<<N$ from a data\n",
    "stream of *unknown* size $N$.\n",
    "\n",
    "Desired prob. for item to be in reservoir $=k/N$ (i.e., **uniformly\n",
    "drawn sample**).\n",
    "\n",
    "Algorithm:\n",
    "\n",
    "-   Store first $k$ items in the reservoir\n",
    "\n",
    "-   Item $i>k$ replaces $j^{th}$ **item in reservoir** if $j$ is random\n",
    "    choice among $[1,i]$ and $j <= k$; otherwise, discard item $i$.\n",
    "\n",
    "> **Exercise:** Prove that reservoir sampling works!\n",
    "\n",
    "## Representative subset for ad-hoc queries\n",
    "\n",
    "Stream consists of tuples with $n$ fields\n",
    "\n",
    "-   Sampling involves only a few of the fields, the *key* values\n",
    "\n",
    "-   If we need a subset that is a fraction $p$ of all the keys:\n",
    "\n",
    "    -   Use a hash function that has $k$ buckets\n",
    "\n",
    "    -   Hashing should **only** involve the **key values**.\n",
    "\n",
    "    -   Choose sample if it hashes to the first $p\\cdot k$ buckets\n",
    "\n",
    "    -   Result will have **all** tuples with certain key values.\n",
    "\n",
    "## Counting Distinct Elements\n",
    "\n",
    "> Count (approximately) the number of **unique** elements, $m$, in a\n",
    "> stream.\n",
    "\n",
    "-   Very large universal set\n",
    "\n",
    "-   Traditional in-core data structure like a hash table or a balanced\n",
    "    search tree is impossible due to space or time constraints.\n",
    "\n",
    "-   But hashing offers a clue: the wider the range (image) of the hash,\n",
    "    the more likely it is that distinct elements will hash to\n",
    "    *distinctive values*.\n",
    "\n",
    "## Experiment\n",
    "\n",
    "Create a sample of size 1000000 drawn from a geometric distribution with\n",
    "probability $p=0.15$ of success.\n",
    "\n",
    "Compute the number of distinct elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd73ef21",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = None\n",
    "distinct_elements = np.unique(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22aac1e2-16fa-4fca-8e84-4233890b4295",
   "metadata": {},
   "source": [
    "## Flajolet-Martin Algorithm\n",
    "\n",
    "Choose a hash function family that maps elements to bit strings of a\n",
    "certain length (ensure that range is much larger than the domain).\n",
    "\n",
    "-   Pick **many hash functions** and apply them to every element.\n",
    "\n",
    "-   **Intuition:**\n",
    "\n",
    "    -   More distinct items $\\implies$ more distinct hash values!\n",
    "\n",
    "    -   For each hash function, count special bit patterns in the image.\n",
    "\n",
    "-   Special pattern: long stretches of trailing zero bits\n",
    "\n",
    "## Estimating $m$\n",
    "\n",
    "**Tail length:** For element $a$ and hash function $h$, the tail length\n",
    "\n",
    "$t_{h}(a) = \\mbox{number of trailing zeros in }h(a)$.\n",
    "\n",
    "-   $R_h$: maximum tail length seen with hash function $h$\n",
    "\n",
    "-   Estimate of $m$ (according to $h$) is $2^{R_h}$.\n",
    "\n",
    "-   **Note:** Prob. that $t_h(a)$ is *at least* $r$ is $2^{-r}$.\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Complete the following definition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "779c1eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tail_length(n):\n",
    "    \"\"\"Returns the number of trailing zeros in the bit representation\n",
    "\n",
    "    n (int): number\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c2b0f9-4f9f-4ab0-a89f-467449f096f9",
   "metadata": {},
   "source": [
    "Use it with a random hash function $h$ drawn from our universal hash\n",
    "family to compute an estimate of the number of unique elements of the\n",
    "geometric random sample.\n",
    "\n",
    "## Justification\n",
    "\n",
    "Suppose that $R_h$ is the current (maximum) tail length and $m$ distinct\n",
    "elements. Is $2^{R_h}$ a good estimate of $m$?\n",
    "\n",
    "-   Prob. that an element has $<r$ tail length $= (1 -\n",
    "      2^{-r})$\n",
    "\n",
    "-   Prob. that all have this property $=(1-2^{-r})^m$, i.e.\n",
    "    $\\approx e^{-m2^{-r}}$.\n",
    "\n",
    "-   Prob. that at least one has tail length $>=r$ is $p=1 -\n",
    "      e^{-m2^{-r}}$.\n",
    "\n",
    "But $p \\rightarrow 1$ (resp. $0$) if $m >> 2^{r}$ (resp. $m << 2^{r}$).\n",
    "\n",
    "## Why Use Many Hashes\n",
    "\n",
    "-   To even out the estimates, e.g. by averaging the estimates from\n",
    "    different hash functions\n",
    "\n",
    "-   Overestimates, in this case, can be problematic: small probability\n",
    "    for $R_h$ increasing by one but estimate doubles!\n",
    "\n",
    "-   Solution:\n",
    "\n",
    "    -   Form large groups of hash functions, with each group large\n",
    "        enough\n",
    "\n",
    "    -   Use the medians of the averages within groups.\n",
    "\n",
    "    -   Space $=$ one tail length per function; elements not stored!\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Simulate the Flajolet-Martin algorithm on the random sample above, using\n",
    "100 hash functions from the universal family to hash the elements.\n",
    "\n",
    "## Interlude: The Majority Element Problem\n",
    "\n",
    "You are given an array of $n$ numbers which contains a **majority**\n",
    "element, i.e. one which has frequency greater than\n",
    "$\\lfloor n/2 \\rfloor$. Find that element!\n",
    "\n",
    "## Algorithm:\n",
    "\n",
    "The algorithm always maintains a **stored element** and an **associated\n",
    "counter**.\n",
    "\n",
    "-   Store the first element and \\*\\*initialize\\* counter to 1\n",
    "-   For every subsequent element:\n",
    "    -   if same as stored element, **increment** counter\n",
    "    -   if different from stored element and counter is zero, replace\n",
    "        stored element with new element and **initialize** counter to 1\n",
    "    -   if different from stored element, **decrement** counter\n",
    "\n",
    "## Counting most frequent elements: Heavy Hitters\n",
    "\n",
    "Generalizes the majority element problem: one version is the **$k$-heavy\n",
    "hitters** problem:\n",
    "\n",
    "> Find the elements that have frequency at least $\\frac{n}{k+1}$.\n",
    "\n",
    "**Observation:** There can be at most $k$ such heavy hitters!\n",
    "\n",
    "## The Algorithm (Gries-Misra)\n",
    "\n",
    "Store upto $k$ elements, each with an associated counter. Initially,\n",
    "nothing is stored and counters are zero.\n",
    "\n",
    "For every element seen:\n",
    "\n",
    "-   if it is currently stored, increment its counter by 1\n",
    "-   if different from all stored elements, and less than $k$ elements\n",
    "    currently stored, add element with counter initialized to 1.\n",
    "-   if different from all stored elements, and $k$ items currently\n",
    "    stored, **decrement** each counter by 1. Remove any element from\n",
    "    store if its count becomes 0 as a result.\n",
    "\n",
    "## Analysis\n",
    "\n",
    "Show that the reported count for each element $t$ in the store is\n",
    "\n",
    "-   lower bounded by $f_t - \\frac{n}{k+1}$\n",
    "-   upper bounded by $f_t$\n",
    "\n",
    "where $f_t$ is its actual frequency.\n",
    "\n",
    "Hence, all heavy hitters will necessarily be stored (but there could be\n",
    "non-hitters that remain in the store).\n",
    "\n",
    "## Exercise\n",
    "\n",
    "Simulate the Gries-Misra algorithm on the random sample above to obtain\n",
    "the $3$-heavy hitters.\n",
    "\n",
    "## Estimating Moments\n",
    "\n",
    "Given a stream over a universal set $S={a_1,\\ldots}$ of distinct\n",
    "elements, let $m_i$ be the number of occurrences of $a_i$.\n",
    "\n",
    "$k^{th}$ Moment of the stream = $\\sum_{i} {m_i}^k$\n",
    "\n",
    "-   $0^{th}$ moment (assuming the convention that $0^0 =\n",
    "      0$) is just the number of distinct elements!\n",
    "\n",
    "-   $1^{st}$ moment is the **total number** of elements in the stream.\n",
    "\n",
    "-   Problem with higher moments: Space limitation, since we cannot keep\n",
    "    exact counts of all the frequencies!\n",
    "\n",
    "-   Let’s see how to estimate second moments.\n",
    "\n",
    "## Alon-Matias-Szegedy Algorithm\n",
    "\n",
    "(Temporary) Assumption: Length of stream known to be $n$\n",
    "\n",
    "**Ingredients**:\n",
    "\n",
    "-   Uniformly sample a small subset of positions in the stream\n",
    "    (*variables* $X_i$)\n",
    "\n",
    "-   Keep track of the frequencies ($f_i$) of the associated elements (at\n",
    "    sampled positions) from the first sampled occurrence onwards.\n",
    "\n",
    "Estimated value for second moment based on variable $X_i$ is\n",
    "$(2\\cdot f_i-1)\\cdot n$\n",
    "\n",
    "## Intuition\n",
    "\n",
    "-   $c_i$ is the frequency *from position $i$ onwards* of element at\n",
    "    position $i$\n",
    "\n",
    "-   Summed over all positions $j$ where a particular element occurs, the\n",
    "    value $\\sum_{j} (2\\cdot c_j -1)$ is the second moment for that\n",
    "    element!\n",
    "\n",
    "-   The multiplicative factor $n$ accounts for the uniform sampling.\n",
    "\n",
    "## Hand-Written Exercise\n",
    "\n",
    "Simulate the AMS second-moment sketch on the sequence\n",
    "\n",
    "\\[6, 4, 6, 11, 11, 4, 2, 1, 8, 8, 15, 2, 4, 7, 5, 4\\]\n",
    "\n",
    "by using the positions 1, 5, 7, 8, 11, 12 (assume that positions are\n",
    "numbered from 1 through 16).\n",
    "\n",
    "## Generalizations\n",
    "\n",
    "-   Can handle higher-order moments $X^k$ by estimating $n(v^k\n",
    "      - (v-1)^k)$ for a small collection of randomly sampled variables\n",
    "\n",
    "-   What if $n$ is not known in advance?\n",
    "\n",
    "    -   Just use $n$ when moments need to reported!\n",
    "\n",
    "    -   Selection of positions can be done using reservoir sampling!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
